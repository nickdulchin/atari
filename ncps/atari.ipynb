{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "#import ale_py\n",
    "import torch\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    ClipRewardEnv,\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    "    MaxAndSkipEnv,\n",
    "    NoopResetEnv,\n",
    ")\n",
    "#from ray.rllib.env.wrappers.atari_wrappers import wrap_deepmind\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ncps.torch import CfC\n",
    "from ncps.datasets.torch import AtariCloningDataset\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 64, 5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, padding=2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 128, 5, padding=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 5, padding=2, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.mean((-1, -2))  # Global average pooling\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvCfC(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv_block = ConvBlock()\n",
    "        self.rnn = CfC(256, 64, batch_first=True, proj_size=n_actions)\n",
    "\n",
    "    def forward(self, x, hx=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        # Merge time and batch dimension into a single one (because the Conv layers require this)\n",
    "        x = x.view(batch_size * seq_len, *x.shape[2:])\n",
    "        x = self.conv_block(x)  # apply conv block to merged data\n",
    "        # Separate time and batch dimension again\n",
    "        x = x.view(batch_size, seq_len, *x.shape[1:])\n",
    "        x, hx = self.rnn(x, hx)  # hx is the hidden state of the RNN\n",
    "        return x, hx\n",
    "\n",
    "\n",
    "def eval(model, valloader):\n",
    "    losses, accs = [], []\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device  # get device the model is located on\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(valloader):\n",
    "            inputs = inputs.to(device)  # move data to same device as the model\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            outputs = outputs.reshape(-1, *outputs.shape[2:])  # flatten\n",
    "            labels = labels.view(-1, *labels.shape[2:])  # flatten\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = (outputs.argmax(-1) == labels).float().mean()\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc.item())\n",
    "\n",
    "            if i >= 3:\n",
    "                break\n",
    "\n",
    "    return np.mean(losses), np.mean(accs)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, criterion, optimizer, trainloader):\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(total=len(trainloader))\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device  # get device the model is located on\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)  # move data to same device as the model\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs, hx = model(inputs)\n",
    "        labels = labels.view(-1, *labels.shape[2:])  # flatten\n",
    "        outputs = outputs.reshape(-1, *outputs.shape[2:])  # flatten\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_description(f\"loss={running_loss / (i + 1):0.4g}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "        if i >= 3:\n",
    "            break\n",
    "    pbar.close()\n",
    "\n",
    "\n",
    "def run_closed_loop(model, env, num_episodes=None):\n",
    "    obs, info = env.reset()\n",
    "    device = next(model.parameters()).device\n",
    "    hx = None  # Hidden state of the RNN\n",
    "    returns = []\n",
    "    total_reward = 0\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            # PyTorch require channel first images -> transpose data\n",
    "            obs = np.transpose(obs, [0, 1, 2]).astype(np.float32)\n",
    "            # Observation seems to be already normalized, see: https://github.com/mlech26l/ncps/issues/48#issuecomment-1572328370\n",
    "            # obs = np.transpose(obs, [2, 0, 1]).astype(np.float32) / 255.0\n",
    "            # add batch and time dimension (with a single element in each)\n",
    "            obs = torch.from_numpy(obs).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            pred, hx = model(obs, hx)\n",
    "            # remove time and batch dimension -> then argmax\n",
    "            action = pred.squeeze(0).squeeze(0).argmax().item()\n",
    "            obs, r, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += r\n",
    "            if done:\n",
    "                obs, info = env.reset()\n",
    "                hx = None  # Reset hidden state of the RNN\n",
    "                returns.append(total_reward)\n",
    "                total_reward = 0\n",
    "                if num_episodes is not None:\n",
    "                    # Count down the number of episodes\n",
    "                    num_episodes = num_episodes - 1\n",
    "                    if num_episodes == 0:\n",
    "                        return returns\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "            env = FireResetEnv(env)\n",
    "        env = ClipRewardEnv(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayScaleObservation(env)\n",
    "        env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def wrap_deepmind(env, dim=84, framestack=True, noframeskip=False):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "\n",
    "    Note that we assume reward clipping is done outside the wrapper.\n",
    "\n",
    "    Args:\n",
    "        env: The env object to wrap.\n",
    "        dim: Dimension to resize observations to (dim x dim).\n",
    "        framestack: Whether to framestack observations.\n",
    "    \"\"\"\n",
    "    env = MonitorEnv(env)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    if env.spec is not None and noframeskip is True:\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = WarpFrame(env, dim)\n",
    "    # env = ScaledFloatFrame(env)  # TODO: use for dqn?\n",
    "    # env = ClipRewardEnv(env)  # reward clipping is handled by policy eval\n",
    "    # 4x image framestacking.\n",
    "    if framestack is True:\n",
    "        env = FrameStack(env, 4)\n",
    "    return env\n",
    "\n",
    "def wrap_deepmind2(env_id, dim=84, capture_video=True, render_mode=None, framestack=True, noframeskip=False):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "\n",
    "    Note that we assume reward clipping is done outside the wrapper.\n",
    "\n",
    "    Args:\n",
    "        env: The env object to wrap.\n",
    "        dim: Dimension to resize observations to (dim x dim).\n",
    "        framestack: Whether to framestack observations.\n",
    "    \"\"\"\n",
    "    if capture_video:# and idx == 0:\n",
    "        env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "        env = gym.wrappers.RecordVideo(env, f\"videos/{env_id}\")\n",
    "    else:\n",
    "        env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    if env.spec is not None and noframeskip is True:\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    #env = WarpFrame(env, dim)\n",
    "    env = gym.wrappers.ResizeObservation(env, (dim, dim))\n",
    "    env = gym.wrappers.GrayScaleObservation(env)\n",
    "    # env = ScaledFloatFrame(env)  # TODO: use for dqn?\n",
    "    # env = ClipRewardEnv(env)  # reward clipping is handled by policy eval\n",
    "    # 4x image framestacking.\n",
    "    if framestack is True:\n",
    "        env = gym.wrappers.FrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/Users/nickdulchin/Documents/local_projects/llms/.venv/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/nickdulchin/Documents/local_projects/atari/videos/ALE/Breakout-v5 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "loss=1.268:   0%|          | 4/938 [01:24<5:29:41, 21.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, val_loss=1.346, val_acc=15.84%\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make(\"ALE/Breakout-v5\")\n",
    "#env = wrap_deepmind(env)\n",
    "# We need to wrap the environment with the Deepmind helper functions\n",
    "env = wrap_deepmind2(\"ALE/Breakout-v5\")\n",
    "#env = make_env(\"ALE/Breakout-v5\", 0, 0, True, \"test\")\n",
    "\n",
    "train_ds = AtariCloningDataset(\"breakout\", split=\"train\")\n",
    "val_ds = AtariCloningDataset(\"breakout\", split=\"val\")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=32, num_workers=4, shuffle=True\n",
    ")\n",
    "valloader = torch.utils.data.DataLoader(val_ds, batch_size=32, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvCfC(n_actions=env.action_space.n).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#for epoch in range(1):  # loop over the dataset multiple times\n",
    "epoch = 0\n",
    "train_one_epoch(model, criterion, optimizer, trainloader)\n",
    "print('finished training')\n",
    "\n",
    "# Evaluate model on the validation set\n",
    "val_loss, val_acc = eval(model, valloader)\n",
    "print(f\"Epoch {epoch+1}, val_loss={val_loss:0.4g}, val_acc={100*val_acc:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean return 1.2 (n=10)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "returns = run_closed_loop(model, env, num_episodes=10)\n",
    "print(f\"Mean return {np.mean(returns)} (n={len(returns)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m returns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'returns' is not defined"
     ]
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickdulchin/Documents/local_projects/llms/.venv/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/nickdulchin/Documents/local_projects/atari/videos/ALE/Breakout-v5/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/nickdulchin/Documents/local_projects/atari/videos/ALE/Breakout-v5/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/nickdulchin/Documents/local_projects/atari/videos/ALE/Breakout-v5/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/nickdulchin/Documents/local_projects/atari/videos/ALE/Breakout-v5/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /Users/nickdulchin/Documents/local_projects/atari/videos/ALE/Breakout-v5/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/nickdulchin/Documents/local_projects/atari/videos/ALE/Breakout-v5/rl-video-episode-1.mp4\n",
      "Mean return 2.2 (n=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Visualize Atari game and play endlessly\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m env \u001b[39m=\u001b[39m wrap_deepmind2(\u001b[39m\"\u001b[39m\u001b[39mALE/Breakout-v5\u001b[39m\u001b[39m\"\u001b[39m, capture_video\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, render_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m run_closed_loop(model, env)\n",
      "\u001b[1;32m/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39m# Observation seems to be already normalized, see: https://github.com/mlech26l/ncps/issues/48#issuecomment-1572328370\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39m# obs = np.transpose(obs, [2, 0, 1]).astype(np.float32) / 255.0\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39m# add batch and time dimension (with a single element in each)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m obs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(obs)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m pred, hx \u001b[39m=\u001b[39m model(obs, hx)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39m# remove time and batch dimension -> then argmax\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m action \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39margmax()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/local_projects/llms/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Separate time and batch dimension again\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(batch_size, seq_len, \u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m x, hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, hx)  \u001b[39m# hx is the hidden state of the RNN\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nickdulchin/Documents/local_projects/atari/atari.ipynb#X31sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x, hx\n",
      "File \u001b[0;32m~/Documents/local_projects/llms/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/local_projects/llms/.venv/lib/python3.11/site-packages/ncps/torch/cfc.py:173\u001b[0m, in \u001b[0;36mCfC.forward\u001b[0;34m(self, input, hx, timespans)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_mixed:\n\u001b[1;32m    172\u001b[0m     h_state, c_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(inputs, (h_state, c_state))\n\u001b[0;32m--> 173\u001b[0m h_out, h_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn_cell\u001b[39m.\u001b[39;49mforward(inputs, h_state, ts)\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_sequences:\n\u001b[1;32m    175\u001b[0m     output_sequence\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(h_out))\n",
      "File \u001b[0;32m~/Documents/local_projects/llms/.venv/lib/python3.11/site-packages/ncps/torch/cfc_cell.py:139\u001b[0m, in \u001b[0;36mCfCCell.forward\u001b[0;34m(self, input, hx, ts)\u001b[0m\n\u001b[1;32m    137\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\u001b[39minput\u001b[39m, hx], \u001b[39m1\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone_layers \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparsity_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     ff1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlinear(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff1\u001b[39m.\u001b[39mweight \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparsity_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff1\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/Documents/local_projects/llms/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1494\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[1;32m   1492\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1495\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply model in closed-loop environment\n",
    "returns = run_closed_loop(model, env, num_episodes=10)\n",
    "print(f\"Mean return {np.mean(returns)} (n={len(returns)})\")\n",
    "\n",
    "# Visualize Atari game and play endlessly\n",
    "env = wrap_deepmind2(\"ALE/Breakout-v5\", capture_video=False, render_mode=\"human\")\n",
    "run_closed_loop(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
